{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple pipelines in `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to see how `scikit-learn` `Pipeline`s can help us to train and deploy machine learning models in an way that is maintainable and resistant to errors! \n",
    "\n",
    "Let's use a classic dataset for the purpose: the prediction of median house prices in regions of Boston as a function of other attributes of the regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "boston = pd.read_csv(\"boston.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0    NaN   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO  LSTAT  target  \n",
       "0     15.3   4.98    24.0  \n",
       "1     17.8   9.14    21.6  \n",
       "2     17.8   4.03    34.7  \n",
       "3     18.7   2.94    33.4  \n",
       "4     18.7   5.33    36.2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for reference, here are the descriptions of the features and target of the dataset:\n",
    "\n",
    "* CRIM - per capita crime rate by town\n",
    "* ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "* INDUS - proportion of non-retail business acres per town.\n",
    "* CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "* NOX - nitric oxides concentration (parts per 10 million)\n",
    "* RM - average number of rooms per dwelling\n",
    "* AGE - proportion of owner-occupied units built prior to 1940\n",
    "* DIS - weighted distances to five Boston employment centres\n",
    "* RAD - index of accessibility to radial highways\n",
    "* TAX - full-value property-tax rate per 10,000 dollars\n",
    "* PTRATIO - pupil-teacher ratio by town\n",
    "* LSTAT - percentage of lower status of the population\n",
    "* target - Median value of owner-occupied homes in 1000's of dollars\n",
    "\n",
    "Our aim will be to develop a **regression model** for the target median price!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many of the values in each column are missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       20\n",
       "ZN          0\n",
       "INDUS      30\n",
       "CHAS        0\n",
       "NOX         0\n",
       "RM          0\n",
       "AGE         0\n",
       "DIS         0\n",
       "RAD         0\n",
       "TAX         0\n",
       "PTRATIO     0\n",
       "LSTAT       0\n",
       "target      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also have a look at some descriptive statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>486.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.653050</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.139538</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.719588</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.881949</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.260420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.689387</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  486.000000  506.000000  476.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.653050   11.363636   11.139538    0.069170    0.554695    6.284634   \n",
       "std      8.719588   23.322453    6.881949    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082268    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.260420    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.689387   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO       LSTAT  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534   12.653063   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946    7.141062   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    1.730000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000    6.950000   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000   11.360000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000   16.955000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000   37.970000   \n",
       "\n",
       "           target  \n",
       "count  506.000000  \n",
       "mean    22.532806  \n",
       "std      9.197104  \n",
       "min      5.000000  \n",
       "25%     17.025000  \n",
       "50%     21.200000  \n",
       "75%     25.000000  \n",
       "max     50.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with the correlation matrix amongst the features and target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CRIM</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.198043</td>\n",
       "      <td>0.423260</td>\n",
       "      <td>-0.053110</td>\n",
       "      <td>0.419520</td>\n",
       "      <td>-0.227405</td>\n",
       "      <td>0.349394</td>\n",
       "      <td>-0.376952</td>\n",
       "      <td>0.623160</td>\n",
       "      <td>0.582676</td>\n",
       "      <td>0.287318</td>\n",
       "      <td>0.458549</td>\n",
       "      <td>-0.389693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN</th>\n",
       "      <td>-0.198043</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.532421</td>\n",
       "      <td>-0.042697</td>\n",
       "      <td>-0.516604</td>\n",
       "      <td>0.311991</td>\n",
       "      <td>-0.569537</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>-0.311948</td>\n",
       "      <td>-0.314563</td>\n",
       "      <td>-0.391679</td>\n",
       "      <td>-0.412995</td>\n",
       "      <td>0.360445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS</th>\n",
       "      <td>0.423260</td>\n",
       "      <td>-0.532421</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055586</td>\n",
       "      <td>0.758235</td>\n",
       "      <td>-0.380082</td>\n",
       "      <td>0.646991</td>\n",
       "      <td>-0.704750</td>\n",
       "      <td>0.585536</td>\n",
       "      <td>0.715208</td>\n",
       "      <td>0.370152</td>\n",
       "      <td>0.593128</td>\n",
       "      <td>-0.466700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS</th>\n",
       "      <td>-0.053110</td>\n",
       "      <td>-0.042697</td>\n",
       "      <td>0.055586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.091203</td>\n",
       "      <td>0.091251</td>\n",
       "      <td>0.086518</td>\n",
       "      <td>-0.099176</td>\n",
       "      <td>-0.007368</td>\n",
       "      <td>-0.035587</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>-0.053929</td>\n",
       "      <td>0.175260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX</th>\n",
       "      <td>0.419520</td>\n",
       "      <td>-0.516604</td>\n",
       "      <td>0.758235</td>\n",
       "      <td>0.091203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.302188</td>\n",
       "      <td>0.731470</td>\n",
       "      <td>-0.769230</td>\n",
       "      <td>0.611441</td>\n",
       "      <td>0.668023</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>-0.427321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM</th>\n",
       "      <td>-0.227405</td>\n",
       "      <td>0.311991</td>\n",
       "      <td>-0.380082</td>\n",
       "      <td>0.091251</td>\n",
       "      <td>-0.302188</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.240265</td>\n",
       "      <td>0.205246</td>\n",
       "      <td>-0.209847</td>\n",
       "      <td>-0.292048</td>\n",
       "      <td>-0.355501</td>\n",
       "      <td>-0.613808</td>\n",
       "      <td>0.695360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE</th>\n",
       "      <td>0.349394</td>\n",
       "      <td>-0.569537</td>\n",
       "      <td>0.646991</td>\n",
       "      <td>0.086518</td>\n",
       "      <td>0.731470</td>\n",
       "      <td>-0.240265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.747881</td>\n",
       "      <td>0.456022</td>\n",
       "      <td>0.506456</td>\n",
       "      <td>0.261515</td>\n",
       "      <td>0.602339</td>\n",
       "      <td>-0.376955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS</th>\n",
       "      <td>-0.376952</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>-0.704750</td>\n",
       "      <td>-0.099176</td>\n",
       "      <td>-0.769230</td>\n",
       "      <td>0.205246</td>\n",
       "      <td>-0.747881</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.494588</td>\n",
       "      <td>-0.534432</td>\n",
       "      <td>-0.232471</td>\n",
       "      <td>-0.496996</td>\n",
       "      <td>0.249929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD</th>\n",
       "      <td>0.623160</td>\n",
       "      <td>-0.311948</td>\n",
       "      <td>0.585536</td>\n",
       "      <td>-0.007368</td>\n",
       "      <td>0.611441</td>\n",
       "      <td>-0.209847</td>\n",
       "      <td>0.456022</td>\n",
       "      <td>-0.494588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>0.464741</td>\n",
       "      <td>0.488676</td>\n",
       "      <td>-0.381626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX</th>\n",
       "      <td>0.582676</td>\n",
       "      <td>-0.314563</td>\n",
       "      <td>0.715208</td>\n",
       "      <td>-0.035587</td>\n",
       "      <td>0.668023</td>\n",
       "      <td>-0.292048</td>\n",
       "      <td>0.506456</td>\n",
       "      <td>-0.534432</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.460853</td>\n",
       "      <td>0.543993</td>\n",
       "      <td>-0.468536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO</th>\n",
       "      <td>0.287318</td>\n",
       "      <td>-0.391679</td>\n",
       "      <td>0.370152</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>-0.355501</td>\n",
       "      <td>0.261515</td>\n",
       "      <td>-0.232471</td>\n",
       "      <td>0.464741</td>\n",
       "      <td>0.460853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.374044</td>\n",
       "      <td>-0.507787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT</th>\n",
       "      <td>0.458549</td>\n",
       "      <td>-0.412995</td>\n",
       "      <td>0.593128</td>\n",
       "      <td>-0.053929</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>-0.613808</td>\n",
       "      <td>0.602339</td>\n",
       "      <td>-0.496996</td>\n",
       "      <td>0.488676</td>\n",
       "      <td>0.543993</td>\n",
       "      <td>0.374044</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.737663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>-0.389693</td>\n",
       "      <td>0.360445</td>\n",
       "      <td>-0.466700</td>\n",
       "      <td>0.175260</td>\n",
       "      <td>-0.427321</td>\n",
       "      <td>0.695360</td>\n",
       "      <td>-0.376955</td>\n",
       "      <td>0.249929</td>\n",
       "      <td>-0.381626</td>\n",
       "      <td>-0.468536</td>\n",
       "      <td>-0.507787</td>\n",
       "      <td>-0.737663</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
       "CRIM     1.000000 -0.198043  0.423260 -0.053110  0.419520 -0.227405  0.349394   \n",
       "ZN      -0.198043  1.000000 -0.532421 -0.042697 -0.516604  0.311991 -0.569537   \n",
       "INDUS    0.423260 -0.532421  1.000000  0.055586  0.758235 -0.380082  0.646991   \n",
       "CHAS    -0.053110 -0.042697  0.055586  1.000000  0.091203  0.091251  0.086518   \n",
       "NOX      0.419520 -0.516604  0.758235  0.091203  1.000000 -0.302188  0.731470   \n",
       "RM      -0.227405  0.311991 -0.380082  0.091251 -0.302188  1.000000 -0.240265   \n",
       "AGE      0.349394 -0.569537  0.646991  0.086518  0.731470 -0.240265  1.000000   \n",
       "DIS     -0.376952  0.664408 -0.704750 -0.099176 -0.769230  0.205246 -0.747881   \n",
       "RAD      0.623160 -0.311948  0.585536 -0.007368  0.611441 -0.209847  0.456022   \n",
       "TAX      0.582676 -0.314563  0.715208 -0.035587  0.668023 -0.292048  0.506456   \n",
       "PTRATIO  0.287318 -0.391679  0.370152 -0.121515  0.188933 -0.355501  0.261515   \n",
       "LSTAT    0.458549 -0.412995  0.593128 -0.053929  0.590879 -0.613808  0.602339   \n",
       "target  -0.389693  0.360445 -0.466700  0.175260 -0.427321  0.695360 -0.376955   \n",
       "\n",
       "              DIS       RAD       TAX   PTRATIO     LSTAT    target  \n",
       "CRIM    -0.376952  0.623160  0.582676  0.287318  0.458549 -0.389693  \n",
       "ZN       0.664408 -0.311948 -0.314563 -0.391679 -0.412995  0.360445  \n",
       "INDUS   -0.704750  0.585536  0.715208  0.370152  0.593128 -0.466700  \n",
       "CHAS    -0.099176 -0.007368 -0.035587 -0.121515 -0.053929  0.175260  \n",
       "NOX     -0.769230  0.611441  0.668023  0.188933  0.590879 -0.427321  \n",
       "RM       0.205246 -0.209847 -0.292048 -0.355501 -0.613808  0.695360  \n",
       "AGE     -0.747881  0.456022  0.506456  0.261515  0.602339 -0.376955  \n",
       "DIS      1.000000 -0.494588 -0.534432 -0.232471 -0.496996  0.249929  \n",
       "RAD     -0.494588  1.000000  0.910228  0.464741  0.488676 -0.381626  \n",
       "TAX     -0.534432  0.910228  1.000000  0.460853  0.543993 -0.468536  \n",
       "PTRATIO -0.232471  0.464741  0.460853  1.000000  0.374044 -0.507787  \n",
       "LSTAT   -0.496996  0.488676  0.543993  0.374044  1.000000 -0.737663  \n",
       "target   0.249929 -0.381626 -0.468536 -0.507787 -0.737663  1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (18, 10))\n",
    "boston['target'].plot(\n",
    " title='Target price distribution', kind = 'hist', ax = axes[0]\n",
    ")\n",
    "\n",
    "boston[boston.columns].mean().plot(\n",
    " title='Means of columns', kind = 'bar', ax = axes[1]\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best practice is to split the full dataset into training and testing sets before we go much further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 12), (506,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = boston.drop(\"target\", axis = 1)\n",
    "y = boston[\"target\"]\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((354, 12), (152, 12), (354,), (152,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most models available in `scikit-learn` will not tolerate data that contains missing values: they will throw an error if we try to fit them to such data. So we need to perform **imputation** of missing values in both training and testing sets. We are going to use the `SimpleImputer` from `scikit-learn` to do this.  \n",
    "  \n",
    "First, let's see a toy example of some data with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     a    b  c\n",
       "0  7.0  2.0  3\n",
       "1  4.0  NaN  6\n",
       "2  NaN  5.0  9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_with_missing = pd.DataFrame([[7, 2, 3], [4, np.nan, 6], [None, 5, 9]], columns = [\"a\", \"b\", \"c\"])\n",
    "data_with_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load and fit a `SimpleImputer` and use it to transform our toy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7. , 2. , 3. ],\n",
       "       [4. , 3.5, 6. ],\n",
       "       [5.5, 5. , 9. ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp_median = SimpleImputer(strategy = 'median')\n",
    "imp_median.fit(data_with_missing)\n",
    "imputed_data = imp_median.transform(data_with_missing)\n",
    "\n",
    "imputed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this in one line using the `fit_transform()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7. , 2. , 3. ],\n",
       "       [4. , 3.5, 6. ],\n",
       "       [5.5, 5. , 9. ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_data = imp_median.fit_transform(data_with_missing)\n",
    "imputed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit and use a simple imputer on our training data. First, how many missing values in the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       12\n",
       "ZN          0\n",
       "INDUS      20\n",
       "CHAS        0\n",
       "NOX         0\n",
       "RM          0\n",
       "AGE         0\n",
       "DIS         0\n",
       "RAD         0\n",
       "TAX         0\n",
       "PTRATIO     0\n",
       "LSTAT       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       0\n",
       "ZN         0\n",
       "INDUS      0\n",
       "CHAS       0\n",
       "NOX        0\n",
       "RM         0\n",
       "AGE        0\n",
       "DIS        0\n",
       "RAD        0\n",
       "TAX        0\n",
       "PTRATIO    0\n",
       "LSTAT      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = imp_median.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train, columns = X.columns)\n",
    "\n",
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `imp_median` object stores the calculated statistics necessary for the imputation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.65535e-01, 0.00000e+00, 8.56000e+00, 0.00000e+00, 5.38000e-01,\n",
       "       6.17050e+00, 7.94500e+01, 3.26745e+00, 5.00000e+00, 3.29500e+02,\n",
       "       1.91000e+01, 1.14900e+01])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_median.statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next on our list of chores, we should probably **scale** our input features. Again, `scikit-learn` offers a nice class to do this: the `StandardScaler`! Let's see the problem we're dealing with, here are the means of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM         3.726399\n",
       "ZN          11.269774\n",
       "INDUS       10.812768\n",
       "CHAS         0.070621\n",
       "NOX          0.556465\n",
       "RM           6.263636\n",
       "AGE         68.934463\n",
       "DIS          3.846197\n",
       "RAD          9.536723\n",
       "TAX        405.443503\n",
       "PTRATIO     18.464689\n",
       "LSTAT       12.756667\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and here are their standard deviations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM         9.150948\n",
       "ZN          23.439633\n",
       "INDUS        6.497864\n",
       "CHAS         0.256554\n",
       "NOX          0.116778\n",
       "RM           0.704269\n",
       "AGE         28.353734\n",
       "DIS          2.132573\n",
       "RAD          8.693396\n",
       "TAX        167.330900\n",
       "PTRATIO      2.222017\n",
       "LSTAT        7.183746\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see they cover a wide range of scales. Let's fit and apply a `StandardScaler` to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train, columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       3.136223e-19\n",
       "ZN        -2.195356e-17\n",
       "INDUS     -1.254489e-16\n",
       "CHAS       2.195356e-18\n",
       "NOX       -6.716222e-16\n",
       "RM         7.661793e-16\n",
       "AGE       -3.443573e-16\n",
       "DIS        9.785016e-17\n",
       "RAD        4.892508e-17\n",
       "TAX        8.530527e-17\n",
       "PTRATIO    1.465243e-15\n",
       "LSTAT     -1.414437e-16\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       1.001415\n",
       "ZN         1.001415\n",
       "INDUS      1.001415\n",
       "CHAS       1.001415\n",
       "NOX        1.001415\n",
       "RM         1.001415\n",
       "AGE        1.001415\n",
       "DIS        1.001415\n",
       "RAD        1.001415\n",
       "TAX        1.001415\n",
       "PTRATIO    1.001415\n",
       "LSTAT      1.001415\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, that looks much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are building a problem for ourselves here in the way that we are coding our data wrangling and preprocessing steps. The main problem is that the code is liable to be quite difficult to **deploy** (by deploy, we mean put into production / hand-off to other colleagues). The `SimpleImputer` and `StandardScaler` have been fitted to the training set, and we will have to keep their fitted parameters with them in deployment.  Also, the code is quite sensitive to the order in which operations are performed. For example, fitting the `StandardScaler` before or after the imputation step will lead to different fitted parameters.\n",
    "\n",
    "We could write our own functions and configuration files to encapsulate these processes, but `scikit-learn` has already beaten us to it with their `Pipeline` class!  \n",
    "  \n",
    "## Transformers and estimators\n",
    "\n",
    "A `Pipeline` is a collection of `Transformer` and `Estimator` objects, but what are these:\n",
    "\n",
    "* A `Transformer` has both `fit()` and `transform()` methods. These transform input data in some way.\n",
    "* An `Estimator` has both `fit()` and `predict()` methods. These use input data to provide a prediction.\n",
    "\n",
    "A `Pipeline` is typically a chain of `Transformer`s followed by an `Estimator`.  \n",
    "  \n",
    "Let's see how we might use a `Pipeline` to write a more encapsulated and maintainable version of the operations we wrote above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer(strategy = \"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our `pipe` as a sequence of `steps`. Each step is passed in as a `tuple` containing a name and a transformer object. Now let's resplit our data so we are back at the start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now we can run `fit_transform()` of the whole pipe on the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.41842092, -0.49799437, -1.07151031, ..., -0.67491325,\n",
       "        -0.83630132, -0.77105976],\n",
       "       [-0.35402254, -0.49799437, -0.74887452, ..., -0.6079123 ,\n",
       "        -0.47831986, -0.13422005],\n",
       "       [-0.23249795, -0.49799437,  1.25897056, ..., -0.02317675,\n",
       "        -1.68650729, -0.70777971],\n",
       "       ...,\n",
       "       [ 0.60241606, -0.49799437,  1.03687709, ...,  1.578755  ,\n",
       "         0.77461525,  0.89711023],\n",
       "       [-0.3796954 , -0.49799437, -0.19364082, ..., -0.62618528,\n",
       "        -0.03084304, -0.36041259],\n",
       "       [ 1.20798494, -0.49799437,  1.03687709, ...,  1.578755  ,\n",
       "         0.77461525, -0.06959361]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Pipeline` stores the parameters and settings neccessary to perform the transformation. Now we could use this pipeline to transform the test data. Note carefully here that we only want to use the pipeline to transform the test data. We do not want to re-fit the pipeline to the test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70123195, -0.49799437,  1.03687709, ...,  1.578755  ,\n",
       "         0.77461525,  0.5510895 ],\n",
       "       [-0.38787897, -0.49799437, -0.22515409, ..., -0.0962687 ,\n",
       "         0.32713843,  1.14349853],\n",
       "       [-0.39078291, -0.49799437,  2.48348595, ...,  1.85284979,\n",
       "         0.72986757,  2.29331062],\n",
       "       ...,\n",
       "       [-0.13259412, -0.49799437,  1.25897056, ..., -0.02317675,\n",
       "        -1.68650729,  0.53897204],\n",
       "       [-0.40363576, -0.49799437, -0.09009724, ..., -0.79064216,\n",
       "         0.05865233,  0.62244786],\n",
       "       [ 0.63337403, -0.49799437,  1.03687709, ...,  1.578755  ,\n",
       "         0.77461525,  0.4824239 ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As earlier, let's just see that the individual steps of the `Pipeline` hold the data necessary for transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.6838e-01, 0.0000e+00, 9.6900e+00, 0.0000e+00, 5.3800e-01,\n",
       "       6.1750e+00, 7.6500e+01, 3.3175e+00, 5.0000e+00, 3.3000e+02,\n",
       "       1.9100e+01, 1.1095e+01])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps.imputer.statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, let's create a baseline model pipeline for the regression task! We'll do this using the `DummyRegressor` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
       "                ('scaler', StandardScaler()), ('regressor', DummyRegressor())])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "pipe_dummy = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer(strategy = \"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"regressor\", DummyRegressor(strategy = \"mean\"))\n",
    "])\n",
    "\n",
    "pipe_dummy.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is this 'dummy' regressor? In a way, it's the simplest possible regression model: predict the **mean** (or median etc) value of `y` regardless of the features fed to it! This is a decent baseline - if our model doesn't outperform the dummy regressor then we are doing something badly wrong. So how does it perform?  \n",
    "\n",
    "Let's calculate the mean absolute error, and remember we mainly want to check performance on the test set (although we also compare with the error on the training set to assess overfitting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train 6.93018449360018\n",
      "MAE test 6.119030627415998\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_train_predict = pipe_dummy.predict(X_train)\n",
    "y_test_predict = pipe_dummy.predict(X_test)\n",
    "print(\"MAE train\", mean_absolute_error(y_train, y_train_predict))\n",
    "print(\"MAE test\", mean_absolute_error(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see this error in the context of the distribution of target prices in the Boston data as a whole! Let's get a boxplot of the prices: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "boston[\"target\"].plot(kind = \"box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit a slightly more ambitious model, i.e. a multiple linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('regressor', LinearRegression())])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pipe_linreg = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer(strategy = \"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"regressor\", LinearRegression())\n",
    "])\n",
    "\n",
    "pipe_linreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train 3.4049581762290533\n",
      "MAE test 3.5274783482361713\n"
     ]
    }
   ],
   "source": [
    "y_train_predict = pipe_linreg.predict(X_train)\n",
    "y_test_predict = pipe_linreg.predict(X_test)\n",
    "print(\"MAE train\", mean_absolute_error(y_train, y_train_predict))\n",
    "print(\"MAE test\", mean_absolute_error(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew! It does better than the dummy pipeline, which is reassuring!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 'kitchen sink' approach - polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our progress so far is encouraging! We've achieved better performance than baseline. We might try to go further and add in additional derived features. One obvious choice is to try **polynomial combinations** of features. So, for example, we might try adding polynomials up to the third power.  \n",
    "\n",
    "Let's think about the `CRIM` variable. Third order polynomials means we add features like: `CRIM^3`, `CRIM^2 * ZN`, `CRIM^2 * INDUS`,... , `CRIM * ZN ^ 2`, `CRIM * INDUS^2`,... plus corresponding columns for all other features and all lower degrees. In other words, we end up with a huge number of derived features!  \n",
    "  \n",
    "Let's see how this performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
       "                ('poly', PolynomialFeatures(degree=3, include_bias=False)),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('regressor', LinearRegression())])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pipe_linreg_poly = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer(strategy = \"median\")),\n",
    "    (\"poly\", PolynomialFeatures(degree = 3, include_bias = False)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"regressor\", LinearRegression())\n",
    "])\n",
    "\n",
    "pipe_linreg_poly.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train 3.2018246183689514e-10\n",
      "MAE test 1136.6667411911712\n"
     ]
    }
   ],
   "source": [
    "y_train_predict = pipe_linreg_poly.predict(X_train)\n",
    "y_test_predict = pipe_linreg_poly.predict(X_test)\n",
    "print(\"MAE train\", mean_absolute_error(y_train, y_train_predict))\n",
    "print(\"MAE test\", mean_absolute_error(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task - 2 mins**  \n",
    "\n",
    "Interpret these MAE values on training and testing sets. What do you think is happening here?\n",
    "\n",
    "**Solution**  \n",
    "\n",
    "It looks like the model is **overfitting** heavily! We get a tiny error on the training set, and a much bigger error on the test set, so we are heavily fitting noise in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularising the regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argh! We think *some* of the polynomial features are probably helpful, but we are being swamped by the number of derived features! How do we fight our way through this maze of possible models to arrive at a decent model that doesn't overfit the training set but still improves on the linear regression without derived features?  \n",
    "  \n",
    "A possible and strong solution to this is the idea of **regularised** regression. Regularisation is the process of **penalising** the regression coefficients for getting too large - we have a penalty called **alpha** that we vary to **tune** the model:\n",
    "\n",
    "* *high* alpha leads to *strong* penalisation of coefficients. This leads to a model with more **bias** (i.e. one that is liable to underfit the training data)\n",
    "* *low* alpha leads to *weak* penalisation of coefficients. This leads to a model with more **variance** (i.e. one that is liable to overfit the training data)\n",
    "\n",
    "The idea is that we treat alpha as a hyperparameter to be tuned using **cross validation**. There are two main algorithms for this: **Lasso regression** and **Ridge regression**. Lasso has the ability to set some regression coefficients to zero, so you can think of it as automatically performing feature selection!\n",
    "\n",
    "Fortunately, `scikit-learn` makes this pretty easy. Let's use an implementation of the Lasso algorithm with cross-validation 'built-in' to it. This model will essentially 'auto-tune' itself to try to select the optimal alpha value! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
       "                ('poly', PolynomialFeatures(degree=3, include_bias=False)),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('regressor', LassoLarsCV(max_iter=50))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoLarsCV\n",
    "\n",
    "pipe_lasso_poly = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer(strategy = \"median\")),\n",
    "    (\"poly\", PolynomialFeatures(degree = 3, include_bias = False)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"regressor\", LassoLarsCV(max_iter = 50))\n",
    "])\n",
    "\n",
    "pipe_lasso_poly.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train 2.2296822723005736\n",
      "MAE test 2.792861184809466\n"
     ]
    }
   ],
   "source": [
    "y_train_predict = pipe_lasso_poly.predict(X_train)\n",
    "y_test_predict = pipe_lasso_poly.predict(X_test)\n",
    "print(\"MAE train\", mean_absolute_error(y_train, y_train_predict))\n",
    "print(\"MAE test\", mean_absolute_error(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat! This looks like it has done an excellent job of improving on the standard linear regression without overfitting. How many possible coefficients were there in the training data with polynomial features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        2.04811516e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -2.60084758e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  1.26188822e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        3.93104027e-01,  0.00000000e+00,  0.00000000e+00, -7.66823676e-01,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -1.34271134e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        2.17962557e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -6.06926880e-02,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -7.78092192e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -2.86273419e-19,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  4.18546126e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -9.47571343e-02,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  2.83280849e-01,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  3.19883012e-02,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -1.01082674e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  5.87724870e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -3.33126200e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -2.50874987e-01,\n",
       "        0.00000000e+00,  0.00000000e+00, -2.91726621e+00, -1.36919655e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -1.27509476e-01,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  8.86492525e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  1.12214508e+00])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lasso_poly.named_steps.regressor.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pipe_lasso_poly.named_steps.regressor.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how many of these possible coefficients were found by the Lasso regression to be non-zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pipe_lasso_poly.named_steps.regressor.coef_ != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can see the Lasso regression has performed some rather drastic feature selection. You could next dig down into these coefficients to find out what features they correspond to etc, look at relative importances etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and deploying models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've found a model we're happy with, and we'd like to save the fitted `Pipeline` to perhaps be shared with others / loaded in to other applications. How can we do this? \n",
    "\n",
    "There are a number of possibilities, but sticking to the Python infrastructure, the easiest way is to **pickle** the pipe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(pipe_lasso_poly, open(\"finalised_model.pkl\", mode = \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we dump the pipeline to a *binary* file. All of the fitted parameters etc. are stored inside this pickle file. Now we might send the file to a colleague working on a different computer and/or perhaps deploy the model as a web service using a web framework like `Flask` or `Django`. \n",
    "\n",
    "Imagine you are on a different computer, and you want to use the model you've been sent. It's as simple as loading it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(\"finalised_model.pkl\", mode = \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's also pretend we have a new set of data we want to run through the model for a prediction of target price. Let's just sample a random row from `X_test` to simulate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>0.0459</td>\n",
       "      <td>52.5</td>\n",
       "      <td>5.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.405</td>\n",
       "      <td>6.315</td>\n",
       "      <td>45.6</td>\n",
       "      <td>7.3172</td>\n",
       "      <td>6.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "288  0.0459  52.5   5.32   0.0  0.405  6.315  45.6  7.3172  6.0  293.0   \n",
       "\n",
       "     PTRATIO  LSTAT  \n",
       "288     16.6    7.6  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = X_test.sample(n = 1, axis = 0)\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the model predict for target price, given this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25.4301381])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying\n",
    "\n",
    "Now we could deploy our model as a simple API using the Flask web server. We've included a file `simple_api.py` that you can run at your command line using `python simple_api.py`. You will now have a server running on your laptop. In `Insomnia`, try sending a `POST` request to http://0.0.0.0:5000/api/ including a JSON body with your data, e.g. `[0.0459, 52.5, 5.32, 0.0, 0.405, 6.315, 45.6, 7.3172, 6.0, 293.0, 16.6, 7.6]`. You should receive a prediction back in the response.\n",
    "\n",
    "In principle, this server code could be run not just on your laptop but in the cloud somewhere. Of course, if you were deploying this properly, you would need to worry about authorising users, using a secured server etc!\n",
    "\n",
    "Here's the code in the server file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from flask import Flask, request, redirect, url_for, flash, jsonify\n",
    "import numpy as np\n",
    "import pickle \n",
    "import json\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "@app.route('/api/', methods=['POST'])\n",
    "def makecalc():\n",
    "    data = request.get_json()\n",
    "    data = np.array(data).reshape(1, -1)\n",
    "    prediction = np.array2string(model.predict(data))\n",
    "\n",
    "    return jsonify(prediction)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    modelfile = 'finalised_model.pkl'\n",
    "    model = pickle.load(open(modelfile, 'rb'))\n",
    "    app.run(debug=True, host='0.0.0.0')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
